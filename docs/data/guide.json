{"categories":[{"category":"Introduction","content":"<p>If you've researched cloud native applications and technologies, you've probably come\nacross the <a href=\"https://cncf.landscape2.io\">CNCF cloud native landscape</a>. Unsurprisingly,\nthe sheer scale of it can be overwhelming. So many categories and so many technologies.\nHow do you make sense of it?</p>\n<p>As with anything else, if you break it down and analyze it one piece at a time, you'll\nfind it's not that complex and makes a lot of sense. In fact, the map is neatly organized\nby functionality and, once you understand what each category represents, navigating it\nbecomes a lot easier.</p>\n<p>In this guide, we'll break this mammoth landscape down and provide a high-level overview\nof its layers, columns, and categories.</p>\n","subcategories":[{"subcategory":"What is the cloud native landscape?","content":"<p>The goal of the cloud native landscape is to compile and organize all cloud native open\nsource projects and proprietary products into categories, providing an overview of the\ncurrent ecosystem. Organizations that have a cloud native project or product can\n<a href=\"https://github.com/cncf/landscape/\">submit a PR</a> to request it to be added to the\nlandscape.</p>\n"},{"subcategory":"How to use this guide","content":"<p>In this guide, you'll find one chapter per layer and column which discusses each category\nwithin it. Categories are broken down into: what it is, the problem it addresses, how it\nhelps, and technical 101. While the first three sections assume no technical background,\nthe technical 101 is targeted to engineers just getting started with cloud native. We\nalso included a section for associated buzzwords and lists CNCF projects.</p>\n<blockquote>\n<h5>INFOBOX</h5>\n<p>When looking at the landscape, you'll note a few distinctions:</p>\n<ul>\n<li><em>Items in large boxes</em> are featured items, and they are used to highlight special\nentries, like CNCF-hosted graduated or incubating projects, or platinum and gold\nmembers.</li>\n<li><em>Items in small white boxes</em> are open source projects. This includes CNCF-hosted\nsandbox projects.</li>\n<li><em>Items in gray boxes</em> are proprietary products.</li>\n</ul>\n<p>Please note that new projects are continuously becoming part of the CNCF so\nalways refer to the actual landscape - things are moving fast!</p>\n</blockquote>\n"},{"subcategory":"Contribute to the CNCF Landscape","content":"<p>Are you searching for an exciting project to contribute to within the CNCF ecosystem?\nLook no further! The CNCF hosts a wide range of projects that span cloud-native computing.\nTo find the perfect project for your skills and interests, check out our comprehensive\ncontribution guide at <a href=\"https://contribute.cncf.io/contributors/getting-started/\">Getting Started</a>.\nIt provides you step-by-step instructions on getting started and offers valuable insights for\nboth newcomers and experienced contributors. Join our vibrant community and make your mark on\ncloud-native innovation today!</p>\n"}]},{"category":"Provisioning","content":"<p>Provisioning is the first layer in the cloud native landscape. It encompasses tools that\nare used to <em>create and harden</em> the foundation on which cloud native apps are built.\nYou'll find tools to automatically configure, create, and manage the infrastructure,\nas well as  for scanning, signing, and storing container images. The layer also extends\nto security with tools that enable policy setting and enforcement, embedded authentication\nand authorization, and the handling of secrets distribution. That's a mouthful, so let's\ndiscuss each category at a time.</p>\n","subcategories":[{"subcategory":"Automation & Configuration","content":"<h4>What it is</h4>\n<p>Automation and configuration tools speed up the creation and configuration of compute\nresources (virtual machines, networks, firewall rules, load balancers, etc.). Tools in\nthis category either handle different parts of the provisioning process or try to control\neverything end-to-end. Most provide the ability to integrate with other projects and\nproducts in the space.</p>\n<h4>Problem it addresses</h4>\n<p>Traditionally, IT processes relied on lengthy and labor intensive manual release cycles,\ntypically between three to six months. Those cycles came with lots of human processes and\ncontrols that slowed down changes to production environments. These slow release cycles\nand static environments aren't compatible with cloud native development. To deliver on\nrapid development cycles, infrastructure must be provisioned dynamically and without\nhuman intervention.</p>\n<h4>How it helps</h4>\n<p>Tools of this category allow engineers to build computing environments without human\nintervention. By codifying the environment setup it becomes reproducible with the click\nof a button. While manual setup is error prone, once codified, environment creation\nmatches the exact desired state -- a huge advantage.</p>\n<p>While tools may take different approaches, they all aim at reducing the required work\nto provision resources through automation.</p>\n<h4>Technical 101</h4>\n<p>As we move from old-style human-driven provisioning to a new on-demand scaling model\ndriven by the cloud, the patterns and tools we used before no longer meet our needs.\nMost organizations can't afford a large 24x7 staff to create, configure, and manage\nservers. Automated tools like Terraform reduce the level of effort required to scale\ntens of servers and networks with hundreds of firewall rules. Tools like Puppet, Chef,\nand Ansible provision and/or configure these new servers and applications\nprogrammatically as they are spun up and allow them to be consumed by developers.</p>\n<p>Some tools interact directly with the infrastructure APIs provided by platforms like\nAWS or vSphere, while others focus on configuring the individual machines to make them\npart of a Kubernetes cluster. Many, like Chef and Terraform, can interoperate to provision\nand configure the environment. Others, like OpenStack, exist to provide an\nInfrastructure-as-a-Service (IaaS) environment that other tools could consume.\nFundamentally, you'll need one or more tools in this space as part of laying down the\ncomputing environment, CPU, memory, storage, and networking, for your Kubernetes clusters.\nYou'll also need a subset of these to create and manage the Kubernetes clusters\nthemselves.</p>\n<p>There are now over 5 CNCF projects in this space, more if you count projects like Cluster\nAPI which don't appear on the landscape. There is also a very robust set of other open\nsource and vendor provided tools.</p>\n","keywords":["Infrastructure-as-Code (IaC)","Automation","Declarative Configuration"]},{"subcategory":"Container Registry","content":"<h4>What it is</h4>\n<p>Before diving into container registries, we need to define three tightly related concepts:</p>\n<ul>\n<li><strong>Container</strong> is &quot;a running process with resource and capability constraints managed by a\ncomputer's operating system&quot;\n(<a href=\"https://github.com/cncf/glossary/blob/main/content/en/container.md\">Cloud Native Glossary</a>).</li>\n<li><strong>Image</strong> is a set of archive files needed to run containers and its process. You could\nsee it as a form of template on which you can create an unlimited number of containers.</li>\n<li><strong>Repository</strong>, or just repo, is a space to store images.</li>\n</ul>\n<p>And <strong>container registries</strong> are specialized web applications that categorize and store repositories.</p>\n<p>Let's recap real quick: images contain the information needed to execute a program\n(within a container) and are stored in repositories which in turn are categorized and\ngrouped in registries. Tools that build, run, and manage containers need access to those\nimages. Access is provided by referencing the registry (the path to access the image).</p>\n<h4>Problem it addresses</h4>\n<p>Cloud native applications are packaged and run as containers. Container registries store and provide\nthe container images needed to run these apps.</p>\n<h4>How it helps</h4>\n<p>By centrally storing all container images in one place, they are easily accessible for any developer\nworking on that app.</p>\n<h4>Technical 101</h4>\n<p>Container registries either store and distribute images or enhance an existing registry in some\nway. Fundamentally, a registry is a web API that allows container runtimes to store and retrieve\nimages. Many provide interfaces to allow container scanning or signing tools to enhance the\nsecurity of the images they store. Some specialize in distributing or duplicating images in a\nparticularly efficient manner. Any environment using containers will need to use one or more\nregistries.</p>\n<p>Tools in this space provide integrations to scan, sign, and inspect the images they store.\nDragonfly and Harbor are CNCF projects and Harbor recently gained the distinction of\n<a href=\"https://goharbor.io/blog/harbor-2.0/\">being the first</a> OCI compliant registry. Each major cloud\nprovider provides its own hosted registry and many other registries can be deployed standalone or\ndirectly into your Kubernetes cluster via tools like Helm.</p>\n","keywords":["Container","OCI Image","Registry"]},{"subcategory":"Security & Compliance","content":"<h4>What it is</h4>\n<p>Cloud native applications are designed to be rapidly iterated on. Think of your mobile phone's\ncontinuous flow of app updates — they evolve everyday, presumably getting better. In order to\nrelease code on a regular cadence you must ensure that the code and operating environment are\nsecure and only accessed by authorized engineers. Tools and projects in this section provide\nsome of the abilities needed to build and run modern applications securely.</p>\n<h4>Problem it addresses</h4>\n<p>Security and compliance tools help harden, monitor, and enforce platform and application security.\nFrom containers to Kubernetes environments, these tools allow you to set policy (for compliance),\nget insights into existing vulnerabilities, catch misconfigurations, and harden the containers and\nclusters.</p>\n<h4>How it helps</h4>\n<p>To run containers securely, containers must be scanned for known vulnerabilities and signed to\nensure they haven't been tampered with. Kubernetes has extremely permissive access control settings\nby default that are unsuitable for production use. The result: Kubernetes clusters are an attractive\ntarget for anyone looking to attack your systems. The tools and projects in this space help harden\nthe cluster and detect when the system is behaving abnormally.</p>\n<h4>Technical 101</h4>\n<ul>\n<li>Audit and compliance</li>\n<li>Path to production:\n<ul>\n<li>Code scanning</li>\n<li>Vulnerability scanning</li>\n<li>Image signing</li>\n</ul>\n</li>\n<li>Policy creation and enforcement</li>\n<li>Network layer security</li>\n</ul>\n<p>Some of these tools are rarely used directly. Trivy, Claire, and Notary, for example, are leveraged\nby registries or other scanning tools. Others represent key hardening components of a modern\napplication platform. Examples include Falco or Open Policy Agent (OPA).</p>\n<p>You'll find a number of mature vendors providing solutions in this space, as well as startups\nfounded explicitly to bring Kubernetes native frameworks to market. At the time of this writing\nFalco, Notary/TUF, and OPA are CNCF projects in this space.</p>\n","keywords":["Image scanning","Image signing","Policy enforcement","Audit","Certificate Management"]},{"subcategory":"Key Management","content":"<h4>What it is</h4>\n<p>Before digging into key management, let's first define cryptographic keys. A key is a string of\ncharacters used to encrypt or sign data. Like a physical key, it locks (encrypts) data so that\nonly someone with the right key can unlock (decrypt) it.</p>\n<p>As applications and operations adapt to a new cloud native world, security tools are evolving to\nmeet new security needs. The tools and projects in this category cover everything from how to\nsecurely store passwords and other secrets (sensitive data such as API keys, encryption keys, etc.)\nto how to safely eliminate passwords and secrets from your microservices environment.</p>\n<h4>Problem it addresses</h4>\n<p>Cloud native environments are highly dynamic, requiring on-demand secret distribution. That means\nit has to be entirely programmatic (no humans in the loop) and automated.</p>\n<p>Additionally, applications need to know if a given request comes from a valid source\n(authentication) and if that request has the right to do whatever it's trying to do\n(authorization). This is commonly referred to as AuthN and AuthZ.</p>\n<h4>How it helps</h4>\n<p>Each tool or project takes a different approach but they all provide a way to either securely\ndistribute secrets and keys or a service or specification related to authentication, authorization,\nor both.</p>\n<h4>Technical 101</h4>\n<p>Tools in this category can be grouped into two sets: 1) key generation, storage, management, and\nrotation, and 2) single sign-on and identity management. Vault, for example, is a rather generic\nkey management tool allowing you to manage different types of keys. Keycloak, on the other hand,\nis an identity broker which can be used to manage access keys for different services.</p>\n","keywords":["AuthN and AuthZ","Identity","Access","Secrets"]},{"subcategory":"Summary","content":"<p>As we've seen, the provisioning layer focuses on building the foundation of your cloud native\nplatforms and applications with tools handling everything from infrastructure provisioning to\ncontainer registries to security. Next, we'll discuss the runtime layer containing cloud native\nstorage, container runtime, and networking.</p>\n"}]},{"category":"Runtime","content":"<p>Now that we've established the foundation of a cloud native environment, we'll move one\ninfrastructure layer up and zoom into the runtime layer. It encompasses everything a container\nneeds to run in a cloud native environment. That includes the code used to start a container,\nreferred to as a container runtime; tools to make persistent storage available to containers;\nand those that manage the container environment networks.</p>\n<p>But note, these resources are not to be confused with the networking and storage work handled by\nthe provisioning layer discussed above. Those focused on getting the container platform running.\nTools in this category are used to start and stop containers, help them store data, and allow them\nto talk to each other.</p>\n","subcategories":[{"subcategory":"Cloud Native Storage","content":"<h4>What it is</h4>\n<p>Storage is where the persistent data of an app is stored, often referred to as a persistent volume.\nTo function reliably, applications need to have easy access to storage. Generally, when we say\npersistent data, we mean storing things like databases, messages, or any other information we want\nto ensure doesn't disappear when an app gets restarted.</p>\n<h4>Problem it addresses</h4>\n<p>Cloud native architectures are fluid, flexible, and elastic, making persisting data between\nrestarts challenging. To scale up and down or self-heal, containerized apps are continuously\ncreated and deleted, changing physical location over time. That's why cloud native storage must\nbe provided node-independently. To store data, though, you'll need hardware, a disk to be specific,\nand disks, just like any other hardware, are infrastructure-bound — our first big challenge.</p>\n<p>Then there is the actual storage interface which can change significantly between datacenters\n(in the old world, each infrastructure had their own storage solution with its own interface),\nmaking portability really tough.</p>\n<p>And lastly, manual provisioning and autoscaling aren't compatible, so, to benefit from the\nelasticity of the cloud, storage must be provisioned automatically.</p>\n<p>Cloud native storage is tailored to this new cloud native reality.</p>\n<h4>How it helps</h4>\n<p>The tools in this category help either:</p>\n<ol>\n<li>Provide cloud native storage options for containers,</li>\n<li>Standardize the interfaces between containers and storage providers, or</li>\n<li>Provide data protection through backup and restore operations.</li>\n</ol>\n<p>The former means storage that uses a cloud native compatible container storage interface\n(tools in the second category) and which can be provisioned automatically, enabling autoscaling\nand self-healing by eliminating the human bottleneck.</p>\n<h4>Technical 101</h4>\n<p>Cloud native storage is largely made possible by the Container Storage Interface (CSI) which\nprovides a standard API for providing file and block storage to containers. There are a number\nof tools in this category, both open source and vendor-provided, that leverage the CSI to provide\non-demand storage for containers.</p>\n<p>Additionally, there are technologies aiming to solve other cloud native storage challenges.\nMinio is a popular project that provides an S3-compatible API for object storage among other\nthings. Tools like Velero help simplify the process of backing up and restoring both the\nKubernetes clusters themselves as well as persistent data used by the applications.</p>\n","keywords":["Persistent volume","CSI","Storage API","Backup and restore"]},{"subcategory":"Container Runtime","content":"<h4>What it is</h4>\n<p>As discussed under container registry, a container is a set of compute constraints used to execute\n(or launch) an application. Containerized apps believe they are running on their own dedicated\ncomputer and are oblivious that they are sharing resources with other processes\n(similar to virtual machines).</p>\n<p>The container runtime is the software that executes containerized (or &quot;constrained&quot;) applications.\nWithout the runtime, you only have the container image, the at-rest file specifying how the\ncontainerized app should look like. The runtime will start an app within a container and provide\nit with the needed resources.</p>\n<h4>Problem it addresses</h4>\n<p>Container images (the files with the application specs) must be launched in a standardized, secure,\nand isolated way. Standardized because you need standard operating rules no matter where they are\nrunning. Secure, well, because you don't want anyone who shouldn't access it to do so. And isolated\nbecause you don't want the app to affect or be affected by other apps (for instance, if a\nco-located application crashes). Isolation basically functions as protection. Additionally, the\napplication needs to be provided resources, such as CPU, storage, and memory.</p>\n<h4>How it helps</h4>\n<p>The container runtime does all that. It launches apps in a standardized fashion across all\nenvironments and sets security boundaries. The latter is where some of these tools differ. Runtimes\nlike CRI-O or gVisor have hardened their security boundaries. The runtime also sets resource limits\nfor the container. Without it, the app could consume resources as needed, potentially taking\nresources away from other apps, so you always need to set limits.</p>\n<h4>Technical 101</h4>\n<p>Not all tools in this category are created equal. Containerd (part of the famous Docker product)\nand CRI-O are standard container runtime implementations. Then there are tools that expand the use\nof containers to other technologies, such as Kata which allows you to run containers as VMs. Others\naim at solving a specific container-related problem such as gVisor which provides an additional\nsecurity layer between containers and the OS.</p>\n","keywords":["Container","MicroVM"]},{"subcategory":"Cloud Native Network","content":"<h4>What it is</h4>\n<p>Containers talk to each other and to the infrastructure layer through a cloud native network.\n<a href=\"https://thenewstack.io/primer-distributed-systems-and-cloud-native-computing/\">Distributed applications</a>\nhave multiple components that use the network for different purposes. Tools in this category create\na virtual network on top of existing networks specifically for apps to communicate, referred to\nas an <strong>overlay network</strong>.</p>\n<h4>Problem it addresses</h4>\n<p>While it's common to refer to the code running in a container as an app, the reality is that most\ncontainers hold only a small specific set of functionalities of a larger application. Modern\napplications such as Netflix or Gmail are composed of a number of these smaller components each\nrunning in its own container. To allow all these independent pieces to function as a cohesive\napplication, containers need to communicate with each other privately. Tools in this category\nprovide that private communication network.</p>\n<p>Data and messages flowing between containers may have sensitive or private data. Because cloud\nnative networking uses software for controlling, inspecting and modifying data flows, it is a lot\neasier to manage, secure and isolate connections between containers. In some cases you may want to\nextend your container networks and network policies such as firewall and access rules to allow an\napp to connect to virtual machines or services running outside the container network. The\nprogrammable and often declarative nature of cloud native networking makes this possible.</p>\n<h4>How it helps</h4>\n<p>Projects and products in this category use the Container Network Interface (CNI), a CNCF project,\nto provide networking functionalities to containerized applications. Some tools, like Flannel, are\nrather minimalist, providing bare bones connectivity to containers. Others, such as NSX-T provide a\nfull software-defined networking layer creating an isolated virtual network for every Kubernetes\nnamespace.</p>\n<p>At a minimum, a container network needs to assign IP addresses to pods (that's where containerized\napps run in Kubernetes), allowing other processes to access it.</p>\n<h4>Technical 101</h4>\n<p>The variety and innovation in this space is largely made possible by the CNI (similar to storage\nand the Container Storage Interface mentioned above).The CNI standardizes the way network layers\nprovide functionalities to pods.  Selecting the right container network for your Kubernetes\nenvironment is critical and you've got a number of tools to choose from. Weave Net, Antrea, Calico,\nand Flannel all provide effective open source networking layers. Their functionalities vary widely\nand your choice should be ultimately driven by your specific needs.</p>\n<p>Numerous vendors support and extend Kubernetes networks with Software Defined Networking (SDN)\ntools, providing additional insights into network traffic, enforcing network policies, and even\nextending container networks and policies to your broader datacenter.</p>\n","keywords":["SDN","Network Overlay","CNI"]},{"subcategory":"Summary","content":"<p>This concludes our overview of the runtime layer which provides all the tools containers need to\nrun in a cloud native environment:</p>\n<ul>\n<li>Cloud native storage gives apps easy and fast access to data needed to run reliably</li>\n<li>The container runtime which creates and starts containers executing application code</li>\n<li>Cloud native networking provides connectivity for containerized apps to communicate.</li>\n</ul>\n"}]},{"category":"Orchestration & Management","content":"<p>Now that we've covered both the provisioning and runtime layer we can now dive into orchestration\nand management. Here you'll find tooling to handle running and connecting your cloud native\napplications. This section covers everything from Kubernetes itself, one of the key enablers of\ncloud native development to the infrastructure layers responsible for inter app, and external\ncommunication. Inherently scalable, cloud native apps rely on automation and resilience, enabled\nby these tools.</p>\n","subcategories":[{"subcategory":"Scheduling & Orchestration","content":"<h4>What it is</h4>\n<p>Orchestration and scheduling refer to running and managing\n<a href=\"https://github.com/cncf/glossary/blob/main/content/en/container.md\">containers</a> across a cluster.\nA cluster is a group of machines, physical or virtual, connected over a network (see cloud native\nnetworking).</p>\n<p>Container orchestrators (and schedulers) are somewhat similar to the operating system (OS) on your\nlaptop. The OS manages all your apps such as Microsoft 365, Slack and Zoom; executes them, and\nschedules when each app gets to use your laptop's hardware resources like CPU, memory and  storage.</p>\n<p>While running everything on a single machine is great, most applications today are a lot bigger\nthan one computer can possibly handle. Think Gmail or Netflix. These massive apps are distributed\nacross multiple machines forming a\n<a href=\"https://thenewstack.io/primer-distributed-systems-and-cloud-native-computing/\">distributed application</a>.\nMost modern-day applications are built this way, requiring software that is able to manage all\ncomponents running across these different machines. In short, you need a &quot;cluster OS.&quot; That's\nwhere orchestration tools come in.</p>\n<p>You probably noticed that containers come up time and again. Their ability to run apps in many\ndifferent environments is key. Container orchestrators, in most cases,\n<a href=\"https://kubernetes.io/\">Kubernetes</a>, provide the ability to manage these containers. Containers\nand Kubernetes are both central to cloud native architectures, which is why we hear so much about\nthem.</p>\n<h4>Problem it addresses</h4>\n<p>As mentioned in the section 'cloud native networking', in cloud native architectures, applications\nare broken down into small components, or services, each placed in a container. You may have heard\nof them referred to as <a href=\"https://github.com/cncf/glossary/blob/main/content/en/microservices-architecture.md\">microservices</a>.\nInstead of having one big app (often known as a 'monolith') you now have dozens or even hundreds\nof (micro)services. And each of these services needs resources, monitoring, and fixing if a problem\noccurs. While it may be feasible to do all those things manually for a single service, you'll need\nautomated processes when dealing with multiple services, each with its own containers.</p>\n<h4>How it helps</h4>\n<p>Container orchestrators automate container management. But what does that mean in practice? Let's\nanswer that for Kubernetes since it is the de facto container orchestrator.</p>\n<p>Kubernetes does something called desired state reconciliation: it matches the current state of\ncontainers within a cluster to the desired state. The desired state is specified by the engineer\n(e.g. ten instances of service A running on three nodes, i.e. machines, with access to database B,\netc.) and continuously compared against the actual state. If the desired and actual state don't\nmatch, Kubernetes reconciles them by creating or destroying objects. For example, if a container\ncrashes, Kubernetes will spin up a new one.</p>\n<p>In short, Kubernetes allows you to treat a cluster as one computer. It focuses only on what that\nenvironment should look like and handles the implementation details for you.</p>\n<h4>Technical 101</h4>\n<p>Kubernetes lives in the orchestration and scheduling section along with other less widely\nadopted orchestrators like Docker Swarm and Mesos. It enables users to manage a number of\ndisparate computers as a single pool of resources in a declarative way.\nDeclarative configuration management in Kubernetes is handled via\n<a href=\"https://kubernetes.io/docs/concepts/architecture/controller/\">control loops</a>, a pattern in which\na process running in Kubernetes monitors the Kubernetes store for a particular object type and\nensures the actual state in the cluster matches the desired state.</p>\n<p>As an example, a user creates a Kubernetes deployment that states there must be 3 copies of a web\napplication. The deployment controller will ensure that those 3 web application containers get\ncreated then continue to monitor the cluster to see if the number of containers is correct. If a\nparticular container gets removed for any reason the deployment controller will cause a new\ninstance to be created. Alternatively if the deployment is modified to scale down to 1 web app\ninstance it will instruct Kubernetes to delete 2 of the running web apps.</p>\n<p>This core controller pattern can also be used to extend Kubernetes by users or software developers.\nThe operator pattern allows people to write custom controllers for custom resources and build any\narbitrary logic, and automation, into kubernetes itself.</p>\n<p>While Kubernetes isn't the only orchestrator the CNCF hosts (both Crossplane and Volcano are\nincubating projects), it is the most commonly used and actively maintained orchestrator.</p>\n","keywords":["Cluster","Scheduler","Orchestration"]},{"subcategory":"Coordination & Service Discovery","content":"<h4>What it is</h4>\n<p>Modern applications are composed of multiple individual services that need to collaborate to\nprovide value to the end user. To collaborate, they communicate over a network (see cloud native\nnetworking), and to communicate, they must first locate one another. Service discovery is the\nprocess of figuring out how to do that.</p>\n<h4>Problem it addresses</h4>\n<p>Cloud native architectures are dynamic and fluid, meaning they are constantly changing. When a\ncontainer crashes on one node, a new container is spun up on a different node to replace it. Or,\nwhen an app scales, replicas are spread out throughout the network. There is no one place where a\nparticular service is — the location of everything is constantly changing. Tools in this category\nkeep track of services within the network so services can find one another when needed.</p>\n<h4>How it helps</h4>\n<p>Service discovery tools address this problem by providing a common place to find and potentially\nidentify individual services. There are basically two types of tools in this category:</p>\n<ol>\n<li><strong>Service discovery engines</strong>: database-like tools that store information on all services and\nhow to locate them</li>\n<li><strong>Name resolution tools</strong>: tools that receive service location requests and return network\naddress information (e.g. CoreDNS)</li>\n</ol>\n<blockquote>\n<h5>INFOBOX</h5>\n<p>In Kubernetes, to make a pod reachable a new abstraction layer called &quot;service&quot;  is introduced.\nServices provide a single stable address for a dynamically changing group of pods.</p>\n<p>Please note that &quot;service&quot; may have different meanings in different contexts, which can be quite\nconfusing. The term &quot;services&quot; generally refers to the service placed inside a container and pod.\nIt's the app component or microservice with a specific function within the actual app, for\nexample your mobile phone's face recognition algorithm.</p>\n<p>A Kubernetes service is the abstraction that helps pods find and connect to each other. It is an\nentry point for a service (functionality) as a collection of processes or pods. In Kubernetes,\nwhen you create a service (abstraction), you create a group of pods which together provide a\nservice (functionality within one or more containers) with a single end point (entry point)\nwhich is the Kubernetes service.</p>\n</blockquote>\n<h4>Technical 101</h4>\n<p>As distributed systems became more and more prevalent, traditional DNS processes and traditional\nload balancers were often unable to keep up with changing endpoint information. To make up for\nthese shortcomings, service discovery tools handle individual application instances rapidly\nregistering and deregistering themselves. Some options such as CoreDNS and etcd are CNCF projects\nand are built into Kubernetes. Others have custom libraries or tools to allow services to operate\neffectively.</p>\n","keywords":["DNS","Service Discovery"]},{"subcategory":"Remote Procedure Call","content":"<h4>What it is</h4>\n<p>Remote Procedure Call (RPC) is a particular technique enabling applications to talk to each other.\nIt's one way of structuring app communication.</p>\n<h4>Problem it addresses</h4>\n<p>Modern apps are composed of numerous individual services that must communicate in order to\ncollaborate. RPC is one option for handling the communication between applications.</p>\n<h4>How it helps</h4>\n<p>RPC provides a tightly coupled and highly opinionated way of handling communication between\nservices. It allows for bandwidth-efficient communications and many programming languages enable\nRPC interface implementations.</p>\n<h4>Technical 101</h4>\n<p>There are a lot of potential benefits with RPC: It makes coding connections easier, it allows for\nextremely efficient use of the network layer and well-structured communications between services.\nRPC has also been criticized for creating brittle connection points and forcing users to do\ncoordinated upgrades for multiple services. gRPC is a particularly popular RPC implementation and\nhas been adopted by the CNCF.</p>\n","keywords":["gRPC"]},{"subcategory":"Service Proxy","content":"<h4>What it is</h4>\n<p>A service proxy is a tool that intercepts traffic to or from a given service, applies some logic to\nit, then forwards that traffic to another service. It essentially acts as a &quot;go-between&quot; that can\ncollect information about network traffic as well as apply rules to it. This can be as simple as\nserving as a load balancer that forwards traffic to individual applications or as complex as\nan interconnected mesh of proxies running side by side with individual containerized applications\nhandling all network connections.</p>\n<p>While a service proxy is useful in and of itself, especially when driving traffic from the broader\nnetwork into a Kubernetes cluster, service proxies are also building blocks for other systems, such\nas API gateways or service meshes, which we'll discuss below.</p>\n<h4>Problem it addresses</h4>\n<p>Applications should send and receive network traffic in a controlled manner. To keep track of the\ntraffic and potentially transform or redirect it, we need to collect data. Traditionally, the code\nenabling data collection and network traffic management was embedded within each application.</p>\n<p>A service proxy &quot;externalizes&quot; this functionality. No longer does it have to live within the app.\nInstead, it's embedded in the platform layer (where your apps run). This is incredibly powerful\nbecause it allows developers to fully focus on writing your value-generating application code,\nallowing the universal task of handling traffic to be managed by the platform team, whose\nresponsibility it should be in the first place. Centralizing the distribution and management of\nglobally needed service functionality such as routing or TLS termination from a single common\nlocation allows communication between services to become more reliable, secure, and performant.</p>\n<h4>How it helps</h4>\n<p>Proxies act as gatekeepers between the user and services or between different services. With this\nunique positioning, they provide insight into what type of communication is happening and can then\ndetermine where to send a particular request or even deny it entirely.</p>\n<p>Proxies gather critical data, manage routing (spreading traffic evenly among services or rerouting\nif some services break down), encrypt connections, and cache content (reducing resource\nconsumption).</p>\n<h4>Technical 101</h4>\n<p>Service proxies work by intercepting traffic between services, applying logic on it, and allowing\nit to move on if permitted. Centrally controlled capabilities embedded into proxies allow\nadministrators to accomplish several things. They can gather detailed metrics about inter-service\ncommunication, protect services from being overloaded, and apply other common standards to\nservices, like mutual TLS. Service proxies are fundamental to other tools like service meshes as\nthey provide a way to enforce higher-level policies to all network traffic.</p>\n<p>Please note, the CNCF includes load balancers and ingress providers into this category.</p>\n","keywords":["Service Proxy","Ingress"]},{"subcategory":"API Gateway","content":"<h4>What it is</h4>\n<p>While humans generally interact with computer programs via a GUI (graphical user interface) such as\na web page or a desktop application, computers interact with each other through APIs\n(application programming interfaces). But an API shouldn't be confused with an API gateway.</p>\n<p>An API gateway allows organizations to move key functions, such as authorizing or limiting the\nnumber of requests between applications, to a centrally managed location. It also functions as a\ncommon interface to (often external) API consumers.</p>\n<h4>Problem it addresses</h4>\n<p>While most containers and core applications have an API, an API gateway is more than just an API.\nAn API gateway simplifies how organizations manage and apply rules to all interactions.</p>\n<p>API gateways allow developers to write and maintain less custom code (the system functionality\nis encoded into the API gateway, remember?). They also enable teams to see and control the\ninteractions between application users and the applications themselves.</p>\n<h4>How it helps</h4>\n<p>An API gateway sits between the users and the application. It acts as a go-between that takes the\nmessages (requests) from the users and forwards them to the appropriate service. But before handing\nthe request off, it evaluates whether the user is allowed to do what they're trying to do and\nrecords details about who made the request and how many requests they've made.</p>\n<p>Put simply, an API gateway provides a single point of entry with a common user interface for app\nusers. It also enables you to handoff tasks otherwise implemented within the app to the gateway,\nsaving developer time and money.</p>\n<blockquote>\n<h5>EXAMPLE</h5>\n<p>Take Amazon store cards. To offer them, Amazon partners with a bank that will issue and manage\nall Amazon store cards. In return, the bank will keep, let's say, $1 per transaction. The bank\nwill use an API gateway to authorize the retailer to request new cards, keep track of the number\nof transactions for billing, and maybe even restrict the number of requested cards per minute.\nAll that functionality is encoded into the gateway, not the services using it. Services just\nworry about issuing cards.</p>\n</blockquote>\n<h4>Technical 101</h4>\n<p>Like proxies and service meshes (see below), an API gateway takes custom code out of our apps and\nbrings it into a central system. The API gateway works by intercepting calls to backend services,\nperforming some kind of value add activity like validating authorization, collecting metrics,\nor transforming requests, then performing whatever action it deems appropriate.</p>\n<p>API gateways serve as a common entry point for a set of downstream applications while at the same\ntime providing a place where teams can inject business logic to handle authorization, rate\nlimiting, and chargeback. They allow application developers to abstract away changes to their\ndownstream APIs from their customers and offload tasks like onboarding new customers to the gateway.</p>\n","keywords":["API Gateway"]},{"subcategory":"Service Mesh","content":"<h4>What it is</h4>\n<p>Service meshes manage traffic (i.e. communication) between services. They enable platform teams\nto add reliability, observability, and security features uniformly across all services running\nwithin a cluster without requiring any code changes.</p>\n<p>Along with Kubernetes, service meshes have become some of the most critical infrastructure\ncomponents of the cloud native stack.</p>\n<h4>Problem it addresses</h4>\n<p>In a cloud native world, we are dealing with multiple services all needing to communicate. This\nmeans a lot more traffic is going back and forth on an inherently unreliable and often slow\nnetwork. To address this new set of challenges, engineers must implement additional functionality.\nPrior to the service mesh, that functionality had to be encoded into every single application.\nThis custom code often became a source of technical debt and provided new avenues for failures\nor vulnerabilities.</p>\n<h4>How it helps</h4>\n<p>Service meshes add reliability, observability, and security features uniformly across all services\non a platform layer without touching the app code. They are compatible with any programming\nlanguage, allowing development teams to focus on writing business logic.</p>\n<blockquote>\n<h5>INFOBOX</h5>\n<p>Since traditionally, these service mesh features had to be coded into each service, each time\na new service was released or updated, the developer had to ensure these features were\nfunctional, too, providing a lot of room for human error. And here's a dirty little secret,\ndevelopers prefer focusing on business logic (value-generating functionalities) rather than\nbuilding reliability, observability, and security features.</p>\n<p>For the platform owners, on the other hand, these are core capabilities and central to everything\nthey do. Making developers responsible for adding features that platform owners need is\ninherently problematic. This, by the way, also applies to general-purpose proxies and API\ngateways mentioned above. Service meshes and API gateways solve that very issue as they are\nimplemented by the platform owners and applied universally across all services.</p>\n</blockquote>\n<h4>Technical 101</h4>\n<p>Service meshes bind all services running on a cluster together via service proxies creating a mesh\nof services, hence service mesh. These are managed and controlled through the service mesh control\nplane. Service meshes allow platform owners to perform common actions or collect data on\napplications without having developers write custom logic.</p>\n<p>In essence, a service mesh is an infrastructure layer that manages inter-service communications by\nproviding command and control signals to a network of service proxies (your mesh). Its power lies\nin its ability to provide key system functionality without having to modify the applications.</p>\n<p>Some service meshes use a general-purpose service proxy (see above) for their data plane. Others\nuse a dedicated proxy; Linkerd, for example, uses the <a href=\"https://linkerd.io/\">Linkerd2-proxy &quot;micro proxy&quot;</a>\nto gain an advantage in performance and resource consumption. These proxies are uniformly attached\nto each service through so-called sidecars. Sidecar refers to the fact that the proxy runs in its\nown container but lives in the same pod. Just like a motorcycle sidecar, it's a separate module\nattached to the motorcycle, following it wherever it goes.</p>\n<blockquote>\n<h5>EXAMPLE</h5>\n<p>Take circuit breaking. In microservice environments, individual components often fail or begin\nrunning slowly. Without a service mesh, developers would have to write custom logic to handle\ndownstream failures gracefully and potentially set cooldown timers to avoid upstream services\nto continually request responses from degraded or failed downstream services. With a service\nmesh, that logic is handled at a platform level.</p>\n<p>Service meshes provide many useful features, including the ability to surface detailed metrics,\nencrypt all traffic, limit what operations are authorized by what service, provide additional\nplugins for other tools, and much more. For more detailed information, check out the\n<a href=\"https://smi-spec.io/\">service mesh interface</a> specification.</p>\n</blockquote>\n","keywords":["Service mesh","Sidecar","Data plane","Control plane"]},{"subcategory":"Summary","content":"<p>As we've seen, tools in this layer deal with how all these independent containerized services are\nmanaged as a group. Orchestration and scheduling tools can be thought of as a  cluster OS\nmanaging containerized applications across your cluster. Coordination and service discovery,\nservice proxies, and service meshes ensure services can find each other and communicate effectively\nin order to collaborate as one cohesive app. API gateways are an additional layer providing even\nmore control over service communication, in particular between external applications. Next, we'll\ndiscuss the application definition and development layer — the last layer of the CNCF landscape. It\ncovers databases, streaming and messaging, application definition, and image build, as well as\ncontinuous integration and delivery.</p>\n"}]},{"category":"App Definition and Development","content":"<p>Everything we have discussed up to this point was related to building a reliable, secure environment\nand providing all needed dependencies. We've now arrived at the top layer of the CNCF cloud\nnative landscape. As the name suggests, the application definition and development layer focuses\non the tools that enable engineers to build apps.</p>\n","subcategories":[{"subcategory":"Database","content":"<h4>What it is</h4>\n<p>A database is an application through which other apps can efficiently store and retrieve data.\nDatabases allow you to store data, ensure only authorized users access it, and enable users to\nretrieve it via specialized requests. While there are numerous different types of databases with\ndifferent approaches, they ultimately all have these same overarching goals.</p>\n<h4>Problem it addresses</h4>\n<p>Most applications need an effective way to store and retrieve data while keeping that data safe.\nDatabases do this in a structured way with proven technology though there is quite a bit of\ncomplexity that goes into doing this well.</p>\n<h4>How it helps</h4>\n<p>Databases provide a common interface for applications to store and retrieve data. Developers use\nthese standard interfaces and a relatively simple query language to store, query, and retrieve\ninformation. At the same time, databases allow users to continuously backup and save data, as\nwell as encrypt and regulate access to it.</p>\n<h4>Technical 101</h4>\n<p>Databases are apps that store and retrieve data, using a common language and interface compatible\nwith a number of different languages and frameworks.</p>\n<p>In general, there are two common types of databases: Structured query language (SQL) databases and\nno-SQL databases. Which database a particular application uses should be driven by its needs and\nconstraints.</p>\n<p>With the rise of Kubernetes and its ability to support stateful applications, we've seen a new\ngeneration of databases take advantage of containerization. These new cloud native databases aim\nto bring the scaling and availability benefits of Kubernetes to databases. Tools like YugabyteDB\nand Couchbase are examples of cloud native databases, although more traditional databases like\nMySQL and Postgres run successfully and effectively in Kubernetes clusters.</p>\n<p>Vitess and TiKV are CNCF projects in this space.</p>\n<blockquote>\n<h5>INFOBOX</h5>\n<p>If you look at this category, you'll notice multiple names ending in DB (e.g. MongoDB,\nCockroachDB, FaunaDB) which, as you may guess, stands for database. You'll also see various\nnames ending in SQL (e.g. MySQL or memSQL) — they are still relevant. Some are &quot;old school&quot;\ndatabases that have been adapted to a cloud native reality. There are also some databases that\nare no-SQL but SQL compatible, such as YugabyteDB and Vitess.</p>\n</blockquote>\n","keywords":["SQL","DB","Persistence"]},{"subcategory":"Streaming & Messaging","content":"<h4>What it is</h4>\n<p>To accomplish a common goal, services need to communicate with one another and keep each other in\nthe loop. Each time a service does something, it sends a message about that particular event.</p>\n<p>Streaming and messaging tools enable service-to-service communication by transporting messages\n(i.e. events) between systems. Individual services connect to the messaging service to either\npublish events, read messages from other services, or both. This dynamic creates an environment\nwhere individual apps are either publishers, meaning they write events, or subscribers that read\nevents, or more likely both.</p>\n<h4>Problem it addresses</h4>\n<p>As services proliferate, application environments become increasingly complex, making the\nmanagement of communication between apps more challenging. A streaming or messaging platform\nprovides a central place to publish and read all the events that occur within a system,\nallowing applications to work together without necessarily knowing anything about one another.</p>\n<h4>How it helps</h4>\n<p>When a service does something other services should know about, it &quot;publishes&quot; an event to the\nstreaming or messaging tool. Services that need to know about these types of events “subscribe”\nand watch the streaming or messaging tool. That's the essence of a publish-subscribe, or just\npub-sub, approach and is enabled by these tools.</p>\n<p>By introducing a &quot;go-between&quot; layer that manages all communication, we are decoupling services\nfrom one another. They simply watch for events, take action, and publish a new one.</p>\n<p>Here's an example. When you first sign up for Netflix, the &quot;signup&quot; service publishes a &quot;new signup\nevent&quot; to a messaging platform with further details such as name, email address, subscription\nlevel, etc. The &quot;account creator&quot; service, which subscribes to signup events, will see the event and\ncreate your account. A &quot;customer communication&quot; service that also subscribes to new signup\nevents will add your email address to the customer mailing list and generate a welcome email,\nand so on.</p>\n<p>This allows for a highly decoupled architecture where services can collaborate without needing to\nknow about one another. This decoupling enables engineers to add new functionality without\nupdating downstream apps, known as consumers, or sending a bunch of queries. The more decoupled a\nsystem is, the more flexible and amenable it is to change. And that is exactly what engineers\nstrive for in a system.</p>\n<h4>Technical 101</h4>\n<p>Messaging and streaming tools have been around long before cloud native became a thing. To\ncentrally manage business-critical events, organizations have built large enterprise service\nbuses. But when we talk about messaging and streaming in a cloud native context, we're generally\nreferring to tools like NATS, RabbitMQ, Kafka, or cloud provided message queues.</p>\n<p>What these systems have in common are the architecture patterns they enable. Application\ninteractions in a cloud native environment are either orchestrated or choreographed. There's a\nlot more to it, but let's just say that orchestrated refers to systems that are centrally managed,\nand choreographed systems allow individual components to act independently.</p>\n<p>Messaging and streaming systems provide a central place for choreographed systems to communicate.\nThe message bus provides a common place where all apps can go to tell others what they're doing\nby publishing messages, or see what's going on by subscribing to messages.</p>\n<p>The NATS and Cloudevents projects are both incubating CNCF projects in this space. NATS provides a\nmature messaging system and Cloudevents is an effort to standardize message formats between\nsystems. Strimzi, Pravega, and Tremor are sandbox projects with each being tailored to a unique\nuse case around streaming and messaging.</p>\n","keywords":["Choreography","Streaming","MQ","Message bus"]},{"subcategory":"Application Definition & Image Build","content":"<h4>What it is</h4>\n<p>Application definition and image build is a broad category that can be broken down into two main\nsubgroups. First, developer-focused tools that help build application code into containers and/or\nKubernetes. And second, operations-focused tools that deploy apps in a standardized way. Whether\nyou intend to speed up or simplify your development environment, provide a standardized way to\ndeploy third-party apps, or wish to simplify the process of writing a new Kubernetes extension,\nthis category serves as a catch-all for a number of projects and products that optimize the\nKubernetes developer and operator experience.</p>\n<h4>Problem it addresses</h4>\n<p>Kubernetes, and containerized environments more generally, are incredibly flexible and powerful.\nWith that flexibility also comes complexity, mainly in the form of multiple configuration options\nas well as multiple demands for the various use cases. Developers need the ability to create\nreproducible images when they containerize their code. Operators need a standardized way to deploy\napps into container environments, and finally, platform teams need to provide tools to simplify\nimage creation and application deployment, both for in-house and third party applications.</p>\n<h4>How it Helps</h4>\n<p>Tools in this space aim to solve some of these developer or operator challenges. On the developer\nside, there are tools that simplify the process of extending Kubernetes to build, deploy, and\nconnect applications. A number of projects and products help to store or deploy pre-packaged apps.\nThese allow operators to quickly deploy a streaming service like NATS or Kafka or install a service\nmesh like Linkerd.</p>\n<p>Developing cloud native applications brings a whole new set of challenges calling for a large set\nof diverse tools to simplify application build and deployments. As you start addressing operational\nand developer concerns in your environment, look for tools in this category.</p>\n<h4>Technical 101</h4>\n<p>Application definition and build tools encompass a huge range of functionality. From extending\nKubernetes to virtual machines with KubeVirt, to speeding app development by allowing you to port\nyour development environment into Kubernetes with tools like Telepresence. At a high level, tools\nin this space solve either developer-focused concerns, like how to correctly write, package, test,\nor run custom apps, or operations-focused concerns, such as deploying and managing applications.</p>\n<p>Helm, the only graduated project in this category, underpins many app deployment patterns. Helm\nallows Kubernetes users to deploy and customize many popular third-party apps, and it has been\nadopted by other projects like the Artifact Hub (a CNCF sandbox project). Companies like Bitnami\nalso provide curated catalogs of apps. Finally, Helm is flexible enough to allow users to customize\ntheir own app deployments and is often used by organizations for their own internal releases.</p>\n<p>The Operator Framework is an incubating project aimed at simplifying the process of building and\ndeploying operators. Operators are out of scope for this guide but let's note here that they help\ndeploy and manage apps, similar to Helm (you can read more about operators\n<a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">here</a>). Cloud Native Buildpacks,\nanother incubating project, aims to simplify the process of building application code into\ncontainers.</p>\n<p>There's a lot more in this space and exploring it all would require a dedicated chapter. But\nresearch these tools further if you want to make Kubernetes easier for developers and operators.\nYou'll likely find something that meets your needs.</p>\n","keywords":["Package Management","Charts","Operators"]},{"subcategory":"Continuous Integration & Delivery","content":"<h4>What it is</h4>\n<p>Continuous integration (CI) and continuous delivery (CD) tools enable fast and efficient development\nwith embedded quality assurance. CI automates code changes by immediately building and testing the\ncode, ensuring it produces a deployable artifact. CD goes one step further and pushes the artifact\nthrough the deployment phases.</p>\n<p>Mature CI/CD systems watch source code for changes, automatically build and test the code, then\nbegin moving it from development to production where it has to pass a variety of tests or validation\nto determine if the process should continue or fail. Tools in this category enable such an approach.</p>\n<h4>Problem it addresses</h4>\n<p>Building and deploying applications is a difficult and error-prone process, particularly when it\ninvolves a lot of human intervention and manual steps. The longer a developer works on a piece of\nsoftware without integrating it into the codebase, the longer it will take to identify an error and\nthe more difficult it will be to fix. By integrating code on a regular basis, errors are caught\nearly and are easier to troubleshoot. After all, finding an error in a few lines of code is a lot\neasier than doing so in a few hundred lines of code, or, even worse, finding it once it reaches\nproduction.</p>\n<p>While tools like Kubernetes offer great flexibility for running and managing apps, they also create\nnew challenges and opportunities for CI/CD tooling. Cloud native CI/CD systems are able to\nleverage Kubernetes itself to build, run, and manage the CI/CD process, often referred to as a\npipeline. Kubernetes also provides information about app health, enabling cloud native CI/CD tools\nto more easily determine if a given change was successful or should be rolled back.</p>\n<h4>How it helps</h4>\n<p>CI tools ensure that any code change or updates developers introduce are built, validated, and\nintegrated with other changes automatically and continuously. Each time a developer adds an update,\nautomated testing is triggered to ensure only good code makes it into the system. CD extends CI to\ninclude pushing the result of the CI process into production-like and production environments.</p>\n<p>Let's say a developer changes the code for a web app. The CI system sees the code change, then\nbuilds and tests a new version of that web app. The CD system takes that new version and deploys\nit into a dev, test, pre-production, and finally production environment. It does that while testing\nthe deployed app after each step in the process. All together these systems represent a CI/CD\npipeline for that web app.</p>\n<h4>Technical 101</h4>\n<p>Over time, a number of tools have been built to help with the process of moving code from a source\ncode repository to production. Like most other areas of computing, the advent of cloud native\ndevelopment has changed CI/CD systems. Some traditional tools like Jenkins, probably the most\nprolific CI tool on the market, have <a href=\"https://jenkins-x.io/\">overhauled</a> themselves entirely to\nbetter fit into the Kubernetes ecosystem. Others, like Flux and Argo have pioneered a new way of\ndoing continuous delivery called GitOps, which the OpenGitOps project is working to define as a\nvendor-neutral standard.</p>\n<p>In general, you'll find projects and products in this space are either (1) CI systems, (2) CD\nsystems, (3) tools that help the CD system decide if the code is ready to be pushed into production,\nor (4), in the case of Spinnaker and Argo, all three. Flux and Argo are CNCF gratuated projects in this\nspace, Keptn is the CNCF incubating project, along with the CNCF sandbox projects\nOpenFeature, OpenGitOps and OpenKruise.\nYou can also find many more options hosted by the\n<a href=\"https://cd.foundation/\">Continuous Delivery Foundation</a>. Look for tools in this space to help\nyour organization automate your path to production.</p>\n","keywords":["CI/CD","Continuous integration","Continuous delivery","Continuous deployment","Blue/green","Canary deploy"]},{"subcategory":"Summary","content":"<p>As we've seen, tools in the application definition and development layer enable engineers to build\ncloud native apps. You'll find databases to store and retrieve data or streaming and messaging\ntools allowing for decoupled, choreographed architectures. Application definition and image build\ntools include a variety of technologies that improve the developer and operator experience.\nFinally, CI/CD helps engineers catch any errors early on, ensuring code is ready for deployment\nby driving up  quality.</p>\n<p>This chapter concludes the layers of the CNCF landscape. Next we'll focus on the observability and\nanalysis &quot;column&quot;.</p>\n"}]},{"category":"Observability and Analysis","content":"<p>Now that we've worked our way through the layers of the CNCF landscape, we'll focus on the columns\nstarting with observability and analysis.</p>\n<p>Before diving into these categories, let's first define observability and analysis. Observability\nis a system characteristic describing the degree to which a system can be understood from its\nexternal outputs. Measured by CPU time, memory, disk space, latency, errors, etc., computer systems\ncan be more or less observable. Analysis is an activity in which you look at this observable data\nand make sense of it.</p>\n<p>To ensure there is no service disruption, you'll need to observe and analyze every aspect of your\napplication so every anomaly gets detected and rectified right away. This is what this category is\nall about. It runs across and observes all layers which is why it's on the side and not embedded\nin a specific layer.</p>\n<p>Tools in this category are broken down into logging, monitoring, tracing, and chaos engineering.\nPlease note that the category name is somewhat misleading — although chaos engineering is listed\nhere, consider it a reliability tool rather than an observability or analysis tool.</p>\n","subcategories":[{"subcategory":"Monitoring","content":"<h4>What it is</h4>\n<p>Monitoring refers to instrumenting an app to collect, aggregate, and analyze logs and metrics to\nimprove our understanding of its behavior. While logs describe specific events, metrics are a\nmeasurement of a system at a given point in time — they are two different things but both necessary\nto get the full picture of your system's health. Monitoring includes everything from watching disk\nspace, CPU usage, and memory consumption on individual nodes to doing detailed synthetic\ntransactions to see if a system or application is responding correctly and in a timely manner.\nThere are a number of different approaches to monitor systems and applications.</p>\n<h4>Problem it addresses</h4>\n<p>When running an application or platform, you want it to accomplish a specific task as designed and\nensure it's only accessed by authorized users. Monitoring allows you to know if it is working\ncorrectly, securely, cost effectively, only accessed by authorized users, as well as any other\ncharacteristic you may be tracking.</p>\n<h4>How it helps</h4>\n<p>Good monitoring allows operators to respond quickly, and even automatically, when an incident\narises. It provides insights into the current health of a system and watches for changes.\nMonitoring tracks everything from application health to user behaviour and is an essential\npart of effectively running applications.</p>\n<h4>Technical 101</h4>\n<p>Monitoring in a cloud native context is generally similar to monitoring traditional applications.\nYou need to track metrics, logs, and events to understand the health of your applications. The\nmain difference is that some of the managed objects are ephemeral, meaning they may not be long\nlasting so tying your monitoring to objects like auto generated resource names won't be a good long\nterm strategy. There are a number of CNCF projects in this space that largely revolve around\nPrometheus, the CNCF graduated project.</p>\n","keywords":["Monitoring","Time series","Alerting","Metrics"]},{"subcategory":"Logging","content":"<h4>What it is</h4>\n<p>Applications emit a steady stream of log messages describing what they are doing at any given time.\nThese log messages capture various events happening in the system such as failed or successful\nactions, audit information, or health events. Logging tools collect, store, and analyze these\nmessages to track error reports and related data. Along with metrics and tracing, logging is one\nof the pillars of observability.</p>\n<h4>Problem it addresses</h4>\n<p>Collecting, storing, and analyzing logs is a crucial part of building a modern platform and\nlogging performs one or all of those tasks. Some tools handle every aspect from collection to\nanalysis while others focus on a single task like collection. All logging tools aim at helping\norganizations gain control over their log messages.</p>\n<h4>How it helps</h4>\n<p>When collecting, storing, and analyzing application log messages, you'll understand what an\napplication was communicating at any given time. But as logs only represent messages that\napplications or platforms deliberately emit, they don't necessarily pinpoint the root cause of a\ngiven issue. That being said, collecting and retaining log messages over time is an extremely\npowerful capability and will help teams diagnose issues and meet regulatory and compliance\nrequirements.</p>\n<h4>Technical 101</h4>\n<p>Collecting, storing, and processing log messages is by no means a new problem, but cloud native\npatterns and Kubernetes have significantly changed the way logs are handled. Some traditional\napproaches to logging that were appropriate for virtual and physical machines, like writing logs\nto a file on a local disk, are ill suited to containerized applications, where  file systems don't\noutlast an application. In a cloud native environment, log collection tools like Fluentd run\nalongside application containers and collect messages directly from the applications. Messages\nare then forwarded on to a central log store to be aggregated and analyzed.</p>\n<p>Fluentd is the only CNCF project in this space.</p>\n","keywords":["Logging"]},{"subcategory":"Tracing","content":"<h4>What it is</h4>\n<p>In a microservices world, services are constantly communicating with each other over the network.\nTracing, a specialized use of logging, allows you to trace the path of a request as it moves\nthrough a distributed system.</p>\n<h4>Problem It addresses</h4>\n<p>Understanding how a microservice application behaves at any given point in time is an extremely\nchallenging task. While many tools provide deep insights into service behavior, it can be difficult\nto tie an action of an individual service to the broader understanding of how the entire app\nbehaves.</p>\n<h4>How it helps</h4>\n<p>Tracing solves this problem by adding a unique identifier to messages sent by the application.\nThat unique identifier allows you to follow (or trace) individual transactions as they move through\nyour system. You can use this information to see the health of your application as well as\ndebug problematic microservices or activities.</p>\n<h4>Technical 101</h4>\n<p>Tracing is a very powerful debugging tool that allows you to troubleshoot and fine tune the\nbehaviour of a distributed application. That power does come at a cost. Application code needs\nto be modified to emit tracing data and any spans (a representation of individual units of work\ndone in a distributed system) need to be propagated by infrastructure components (e.g. service\nmeshes and their proxies) in the data path of your application. Jaeger and Open Tracing are CNCF\nprojects in this space.</p>\n","keywords":["Span","Tracing"]},{"subcategory":"Chaos Engineering","content":"<h4>What it is</h4>\n<p>Chaos engineering refers to the practice of intentionally introducing faults into a system in\norder to test its resilience and ensure applications and engineering teams are able to withstand\nturbulent and unexpected events. A chaos engineering tool will provide a controlled way to\nintroduce faults and run specific experiments against a particular instance of an application.</p>\n<h4>Problem it addresses</h4>\n<p>Complex systems fail. They fail for a host of reasons and in a distributed system the consequences\nare typically hard to understand. Chaos engineering is embraced by organizations that accept that\nfailures will occur and, instead of trying to prevent failures, practice recovering from them.\nThis is referred to as optimizing for\n<a href=\"https://en.wikipedia.org/wiki/Mean_time_to_repair\">mean time to repair</a>, or MTTR.</p>\n<blockquote>\n<h5>INFOBOX</h5>\n<p>The traditional approach to maintaining high availability for applications is referred to as\noptimizing for <a href=\"https://en.wikipedia.org/wiki/Mean_time_between_failures\">mean time between failures</a>,\nor MTBF. You can observe this practice in organizations that use things like &quot;change review\nboards&quot; and &quot;long change freezes&quot; to keep an application environment stable by restricting\nchanges. The authors of <a href=\"https://itrevolution.com/accelerate-book/\">Accelerate</a> suggest that\nhigh performing IT organizations achieve high availability by optimizing for mean time to\nrecovery, or MTTR, instead.</p>\n</blockquote>\n<h4>How it Helps</h4>\n<p>In a cloud native world, applications must dynamically adjust to failures, a relatively new\nconcept. That means, when something fails, the system doesn't go down completely but gracefully\ndegrades or recovers. Chaos engineering tools enable you to experiment on a software system in\nproduction to ensure they perform gracefully should a real failure occur.</p>\n<p>In short, you experiment with a system because you want to be confident that it can withstand\nturbulent and unexpected conditions. Instead of waiting for something to happen and find out, you\nplace it under duress in controlled conditions to identify weaknesses and fix them before chance\nuncovers them for you.</p>\n<h4>Technical 101</h4>\n<p>Chaos engineering tools and practices are critical to achieving high availability for your\napplications. Distributed systems are often too complex to be fully understood by any one engineer\nand no change process can fully predetermine the impact of changes on an environment. By\nintroducing deliberate chaos engineering practices teams are able to practice and automate\nfailure recovery. Chaos Mesh and Litmus Chaos are two CNCF tools in this space.</p>\n","keywords":["Chaos Engineering"]},{"subcategory":"Summary","content":"<p>As we've seen, the observability and analysis column is all about understanding the health of your\nsystem and ensuring it stays operational even under tough conditions. Logging tools capture event\nmessages emitted by apps, monitoring watches logs and metrics, and tracing follows the path of\nindividual requests. When combined, these tools ideally provide a 360 degree view of what's going\non within your system. Chaos engineering is a little different. It provides a safe way to verify\nthe system can withstand unexpected events, ensuring it stays healthy.</p>\n<p>Next, we'll focus on cloud native platforms. Configuring tools across the landscape so they work\nwell together is no easy task. Platforms bundle them together, easing adoption.</p>\n"}]},{"category":"Platform","content":"<p>As we've seen so far, each of the categories discussed solves a particular problem. Storage alone\ndoes not provide all you need to manage your app. You'll need an orchestration tool, a container\nruntime, service discovery, networking, an API gateway, etc. Platforms bundle different tools from\ndifferent layers together, solving a larger problem.</p>\n<p>There isn't anything inherently new in these platforms. Everything they do can be done by one of\nthe tools in these layers or the observability and analysis column. You could certainly build your\nown platform and, in fact, many organizations do. However, configuring and fine-tuning the different\nmodules reliably and securely while ensuring that all technologies are always kept up to date\nand vulnerabilities patched is no easy task—you'll need a dedicated team to build and maintain it.\nIf you don't have the necessary resources or know-how, your team is likely better off with a\nplatform. For some organizations, especially those with small engineering teams, platforms are the\nonly way to adopt a cloud native approach.</p>\n<p>You'll probably notice, all platforms revolve around\n<a href=\"https://github.com/cncf/glossary/blob/main/content/en/kubernetes.md\">Kubernetes</a>. That's because\nis at the core of the cloud native stack.</p>\n","subcategories":[{"subcategory":"Certified Kubernetes - Distribution","content":"<h4>What it is</h4>\n<p>A distribution, or distro, is when a vendor takes core Kubernetes — that's the unmodified, open\nsource code (although some modify it) — and packages it for redistribution. Usually this entails\nfinding and validating the Kubernetes software and providing a mechanism to handle cluster\ninstallation and upgrades. Many Kubernetes distributions include other proprietary or open source\napplications.</p>\n<h4>What it addresses</h4>\n<p><a href=\"https://github.com/kubernetes/kubernetes\">Open source Kubernetes</a> doesn't specify a particular\ninstallation tool and leaves many setup configuration choices to the user. Additionally, there is\nlimited support for issues as they arise through community resources like\n<a href=\"https://discuss.kubernetes.io/\">Community Forums</a>,\n<a href=\"https://stackoverflow.com/questions/tagged/kubernetes\">StackOverflow</a>, or\n<a href=\"https://slack.k8s.io/\">Slack</a>.</p>\n<p>While using Kubernetes has become easier over time, it can be challenging to find and use the open\nsource installers. Users need to understand what versions to use, where to get them, and if a\nparticular component is compatible with another. They also need to decide what software will be\ndeployed to their clusters and what settings to use to ensure their platforms are secure, stable,\nand efficient. All this requires deep Kubernetes expertise that may not be readily available\nin-house.</p>\n<h4>How it helps</h4>\n<p>Kubernetes distributions provide a trusted and reliable way to install Kubernetes and provide\nopinionated defaults that create a better and more secure operating environment. A Kubernetes\ndistribution gives vendors and projects the control and predictability they need to provide support\nfor a customer as they go through the lifecycle of deploying, maintaining, and upgrading their\nKubernetes clusters.</p>\n<p>That predictability enables distribution providers to support users when they have production\nissues. Distributions also often provide a tested and supported upgrade path that allows users\nto keep their Kubernetes clusters up to date. Additionally, distributions often provide software\nto deploy on top of Kubernetes that makes it easier to use.</p>\n<p>Distributions significantly ease and speed up Kubernetes adoption. Since the expertise needed to\nconfigure and fine-tune the clusters is coded into the platform, organizations can get up and\nrunning with cloud native tools without having to hire additional engineers with specialized\nexpertise.</p>\n<h4>Technical 101</h4>\n<p>If you've installed Kubernetes, you've likely used something like kubeadm to get your cluster up\nand running. Even then, you probably had to decide on a CNI, install, and configure it. Then, you\nmight have added some storage classes, a tool to handle log messages, maybe an ingress controller,\nand the list goes on. A Kubernetes distribution will automate some or all of that setup. It will\nalso ship with configuration settings based on its own interpretation of best practice or an\nintelligent default. Additionally, most distributions will come with some extensions or add-ons\nbundled and tested to ensure you can get going with your new cluster as quickly as possible.</p>\n<p>There are a lot of options in this category. <a href=\"https://k3s.io/\">k3s</a> is the only CNCF project in\nthis category. There are a lot of great open source and commercial options available. We encourage\nyou to think carefully about your needs when you begin evaluating distributions.</p>\n"},{"subcategory":"Certified Kubernetes - Hosted","content":"<h4>What it is</h4>\n<p>Hosted Kubernetes is a service offered by infrastructure providers like AWS, Digital Ocean, Azure,\nand Google, allowing customers to spin up a Kubernetes cluster on-demand. The cloud provider\ntakes responsibility for managing part of the Kubernetes cluster, usually called the control plane.\nThey are similar to distributions but managed by the cloud provider on their infrastructure.</p>\n<h4>Problem it addresses</h4>\n<p>Hosted Kubernetes allows teams to get started with Kubernetes without knowing or doing anything\nbeyond setting up an account with a cloud vendor. It solves four of the five Ws of getting started\nwith Kubernetes. Who (manages it): your cloud provider; what: their hosted Kubernetes offering;\nwhen: now; and where: on the cloud providers infrastructure. The why is up to you.</p>\n<h4>How it Helps</h4>\n<p>Since the provider takes care of all management details, hosted Kubernetes is the easiest way to\nget started with cloud native. All users have to do is develop their apps and deploy them on the\nhosted Kubernetes services — it's incredibly convenient. Hosted Kubernetes allows users to spin up\na cluster and get started right away (with the exception of AWS' EKS which also requires users to\ntake some additional steps to prepare their clusters) while taking some responsibility for the\ncluster availability. It's worth noting that with the extra convenience of these services comes\nsome reduced flexibility. The offering is bound to the cloud provider, and Kubernetes users don't\nhave access to the control plane.</p>\n<h4>Technical 101</h4>\n<p>Hosted Kubernetes are on-demand Kubernetes clusters provided by a vendor, usually an infrastructure\nhosting provider. The vendor takes responsibility for provisioning the cluster and managing the\nKubernetes control plane. Again, the notable exception is EKS, where individual node provisioning\nis left up to the client.</p>\n<p>Hosted Kubernetes allows an organization to quickly provision new clusters and reduce their\noperational risk by outsourcing infrastructure component management to another organization. The\nmain trade-offs are that you'll likely be charged for the control plane management and that you'll\nbe limited in what you can do. Managed clusters provide stricter limits on configuring your\nKubernetes cluster than DIY Kubernetes clusters.</p>\n","keywords":["Hosted"]},{"subcategory":"Certified Kubernetes - Installer","content":"<h4>What it is</h4>\n<p>Kubernetes installers help install Kubernetes on a machine. They automate the Kubernetes\ninstallation and configuration process and may even help with upgrades. Kubernetes installers\nare often coupled with or used by Kubernetes distributions or hosted Kubernetes offerings.</p>\n<h4>Problem it addresses</h4>\n<p>Similar to Kubernetes distributions, Kubernetes installers simplify getting started with\nKubernetes. Open source Kubernetes relies on installers like kubeadm which, as of this writing,\nis part of the Certified Kubernetes Administrator certification exam to get Kubernetes clusters\nup and running.</p>\n<h4>How it helps</h4>\n<p>Kubernetes installers ease the Kubernetes installation process. Like distributions, they provide a\nvetted source for the source code and version. They also often ship with opinionated Kubernetes\nenvironment configurations. Kubernetes installers like <a href=\"https://kind.sigs.k8s.io/\">kind</a>\n(Kubernetes in Docker) allow you to get a Kubernetes cluster with a single command.</p>\n<h4>Technical 101</h4>\n<p>Whether you're installing Kubernetes locally on Docker, spinning up and provisioning new virtual\nmachines, or preparing new physical servers, you're going to need a tool to handle all the\npreparation of various Kubernetes components (unless you're looking to do it the\n<a href=\"https://github.com/kelseyhightower/kubernetes-the-hard-way\">hard way</a>).</p>\n<p>Kubernetes installers simplify that process. Some handle spinning up nodes and others merely\nconfigure nodes you've already provisioned. They all offer various levels of automation and each\nsuits different use cases. When getting started with an installer, start by understanding your\nneeds, then pick an installer that addresses them. At the time of this writing, kubeadm is\nconsidered so fundamental to the Kubernetes ecosystem that it's included as part of the CKA,\ncertified Kubernetes administrator exam. Minikube, kind, kops, and kubespray are all CNCF-owned\nKubernetes installer projects.</p>\n","keywords":["Installer"]},{"subcategory":"PaaS/Container Service","content":"<h4>What it is</h4>\n<p>A Platform-as-a-Service, or PaaS, is an environment that allows users to run applications\nwithout necessarily concerning themselves with the details of the underlying compute resources.\nPaaS and container services in this category are mechanisms to either host a PaaS for developers\nor host services they can use.</p>\n<h4>Problem it addresses</h4>\n<p>We've talked a lot about the tools and technologies around cloud native. A PaaS attempts to\nconnect many of the technologies found in this landscape in a way that provides direct value\nto developers. It answers the following questions: how will I run applications in various\nenvironments? And, once running, how will my team and users interact with them?</p>\n<h4>How it helps</h4>\n<p>PaaS provides opinions and choices around how to piece together the various open and closed\nsource tools needed to run applications. Many offerings include tools that handle PaaS installation\nand upgrades and the mechanisms to convert application code into a running application.\nAdditionally, PaaS handles the runtime needs of application instances, including on-demand\nscaling of individual components and visibility into the performance and log messages of\nindividual apps.</p>\n<h4>Technical 101</h4>\n<p>Organizations are adopting cloud native technologies to achieve specific business or\norganizational objectives. A PaaS provides a quicker path to value than building a custom\napplication platform. Tools like Cloud Foundry Application Runtime help organizations get up\nand running with new applications quickly. They excel at providing the tools needed to\nrun <a href=\"https://12factor.net/\">12 factor</a> or cloud native applications.</p>\n<p>Any PaaS comes with its own set of trade-offs and restrictions. Most only work with a subset of\nlanguages or application types and the opinions and decisions baked into these platforms may or\nmay not be a good fit for your needs. Stateless applications tend to do very well in a PaaS but\nstateful applications like databases usually don't. There are currently no CNCF projects in this\nspace but most of the offerings are open source and Cloud Foundry is managed by the Cloud Foundry\nFoundation.</p>\n"},{"subcategory":"Summary","content":"<p>As we've seen there are multiple tools that help ease Kubernetes adoption. From Kubernetes\ndistributions and hosted Kubernetes to more barebones installers or PaaS, they all take various\ninstallation and configuration burdens and pre-package them for you. Each solution comes with its\nown &quot;flavor&quot;. Vendor opinions about what's important and appropriate are built into the solution.</p>\n<p>Before adopting any of these, you'll need to do some research to identify the best solution for\nyour particular use case. Will you likely encounter advanced Kubernetes scenarios where you'll need\ncontrol over the control plane? If so, hosted solutions may not be a good fit. Do you have a small\nteam that manages &quot;standard&quot; workloads and needs to offload as many operational tasks as possible?\nThere are multiple aspects to consider. While there is no single best tool for all use cases,\nthere certainly will be an optimal tool for your needs.</p>\n"}]},{"category":"Batch Scheduling","content":"<p>Placeholder</p>\n","subcategories":[{"subcategory":"Kubernetes-Sponsored","content":"<h4>Placeholder</h4>\n<p>Placeholder</p>\n","keywords":["test"]},{"subcategory":"Kubernetes-Native","content":"<p>Placeholder</p>\n"},{"subcategory":"K8s-Connectors","content":"<p>Placeholder</p>\n"}]},{"category":"Summary","content":"<p>Now that we've broken the CNCF Cloud Native Landscape down and discussed it layer by layer,\ncategory by category, it probably feels less overwhelming. There is a logical structure to it and,\nonce you understand it, navigating the landscape becomes a lot easier.</p>\n<p>The layers of the CNCF Landscape build on each other. First, there is the <strong>provisioning</strong> layer\nwith the tools needed to lay the infrastructure foundation. Next is the <strong>runtime</strong> layer where\neverything revolves around containers and what they need to run in a cloud native environment.\nThe <strong>orchestration and management</strong> layer contains the tools to orchestrate and manage your\ncontainers and applications — in other words, the tools needed to create the platform on which\napplications are built. The <strong>application and definition and development</strong> layer is concerned with the tooling\nneeded to enable applications to store and send data as well as with the ways we build and\ndeploy our applications.</p>\n<p>Next to the layers, there are two columns. The <strong>observability and analysis</strong> column includes\ntools that monitor applications and flag when something is wrong. Since all layers have to be\nmonitored, this category runs across all of them. And finally, there are <strong>platforms</strong>. Platforms\ndon't provide new functionality, instead, they bundle multiple tools across the different layers\ntogether, configuring and fine-tuning them so they are ready to be used. This eases the adoption of\ncloud native technologies and may even be the only way organizations are able to leverage them.</p>\n<p>This concludes the CNCF Landscape guide. We hope you enjoyed the read and that we were able to\nbring a little more clarity to the landscape.</p>\n<blockquote>\n<h5>NOTE</h5>\n<p>The cloud native space evolves quickly. If you see anything that's outdated, please submit a PR\nso we can update it. We want this to be a living document and appreciate your contribution.</p>\n</blockquote>\n"}]}